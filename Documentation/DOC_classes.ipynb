{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Directory of clean_cresci_2015.py to sys.path\n",
    "sys.path.append(os.path.abspath(\"git/clean_cresci_2015.py\"))\n",
    "\n",
    "# import clean_cresci_2015\n",
    "from clean_cresci_2015 import clean_cresci_2015\n",
    "from import_data import ImportData\n",
    "from evaluation import Evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Cresci-2015 Dataset\n",
    "\n",
    "## Overview\n",
    "This script is designed to clean and preprocess the Cresci-2015 dataset, which consists of Twitter data related to social bots and genuine users. The script performs various data cleaning and preprocessing steps to prepare the data for further analysis or machine learning tasks.\n",
    "\n",
    "## Dependencies\n",
    "- pandas: Data manipulation library in Python.\n",
    "- numpy: Numerical computing library in Python.\n",
    "- os: Operating system interface for file operations.\n",
    "- datetime: Library for manipulating dates and times in Python.\n",
    "- sklearn.preprocessing.MinMaxScaler: Class for scaling numerical features to a specified range.\n",
    "- sklearn.model_selection.train_test_split: Function for splitting data into training and testing sets.\n",
    "\n",
    "## Class: clean_cresci_2015\n",
    "This class contains a method `clean_data` which performs the following steps:\n",
    "\n",
    "1. **Loading Data**: \n",
    "   - The script loads tweet and user data from CSV files located in subdirectories of the base directory specified as `base_directory`.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Selects relevant features from both tweet and user datasets.\n",
    "\n",
    "3. **Data Type Conversion**:\n",
    "   - Converts the 'created_at' column in the users dataset to datetime format.\n",
    "\n",
    "4. **Handling Missing Values**:\n",
    "   - Fills missing values with zeros for numeric columns in both tweet and user datasets.\n",
    "\n",
    "5. **Feature Engineering**:\n",
    "   - Calculates additional features such as 'account_age_years', 'followers_to_friends_ratio' in the users dataset.\n",
    "   - Calculates tweet-level features such as 'retweet_ratio' and 'reply_ratio'.\n",
    "\n",
    "6. **Normalization**:\n",
    "   - Scales numeric features in both tweet and user datasets to a range between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "7. **Data Merging**:\n",
    "   - Merges the tweet and user datasets on the 'user_id' and 'id' columns respectively.\n",
    "\n",
    "8. **Bot Labeling**:\n",
    "   - Adds a binary 'bot' label based on the folder name.\n",
    "\n",
    "9. **Saving Cleaned Data**:\n",
    "   - Saves the cleaned and processed dataframes as CSV files in a 'clean' subdirectory within each dataset's folder.\n",
    "\n",
    "## Example Usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = clean_cresci_2015()\n",
    "cleaner.clean_data(base_directory=\"git/cresci-2015.csv/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics and Visualization\n",
    "\n",
    "## Overview\n",
    "This script defines a class `Evaluate` that computes various evaluation metrics for binary classification models and provides methods to visualize the evaluation results.\n",
    "\n",
    "## Dependencies\n",
    "- pandas: Data manipulation library in Python.\n",
    "- numpy: Numerical computing library in Python.\n",
    "- matplotlib.pyplot: Plotting library in Python.\n",
    "- seaborn: Statistical data visualization library based on matplotlib.\n",
    "- sklearn.metrics: Collection of metrics to evaluate the performance of machine learning models.\n",
    "\n",
    "## Class: Evaluate\n",
    "This class contains methods to compute the following evaluation metrics:\n",
    "\n",
    "1. **Accuracy**: Computes the accuracy score of the predictions.\n",
    "2. **Confusion Matrix**: Computes the confusion matrix rates (True Negative Rate, False Positive Rate, False Negative Rate, True Positive Rate).\n",
    "3. **Precision**: Computes the precision score.\n",
    "4. **Recall**: Computes the recall score.\n",
    "5. **F1 Score**: Computes the F1 score, which is the harmonic mean of precision and recall.\n",
    "6. **Matthews Correlation Coefficient (MCC)**: Computes the MCC, which measures the correlation between predicted and true binary classifications.\n",
    "7. **Area Under the ROC Curve (AUC)**: Computes the AUC score, which measures the area under the Receiver Operating Characteristic (ROC) curve.\n",
    "\n",
    "Additionally, the class provides methods to:\n",
    "- Get all evaluation metrics at once.\n",
    "- Plot the confusion matrix.\n",
    "- Plot the ROC curve.\n",
    "\n",
    "## Usage\n",
    "1. Instantiate the `Evaluate` class with true values and predicted values.\n",
    "2. Optionally, provide predicted probabilities for computing AUC.\n",
    "3. Call individual metric methods or use `get_all_metrics` to obtain all metrics at once.\n",
    "4. Use `plot_confusion_matrix` to visualize the confusion matrix.\n",
    "5. Use `plot_roc_curve` to visualize the ROC curve.\n",
    "\n",
    "## Example Usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = [0, 1, 1, 0, 1, 0, 0, 1]\n",
    "predicted_values = [0, 1, 1, 0, 1, 0, 1, 1]\n",
    "predicted_probabilities = [0.1, 0.9, 0.7, 0.2, 0.8, 0.3, 0.6, 0.4]\n",
    "\n",
    "evaluator = Evaluate(true_values, predicted_values, predicted_probabilities)\n",
    "print(evaluator.get_all_metrics())\n",
    "\n",
    "evaluator.plot_confusion_matrix()\n",
    "evaluator.plot_roc_curve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Splitting\n",
    "\n",
    "## Overview\n",
    "This script provides functionality for importing data from the Cresci-2015 dataset and splitting it into training, testing, and validation sets. It also allows for sampling the data based on specified bot ratios.\n",
    "\n",
    "## Dependencies\n",
    "- os: Operating system interface for file operations.\n",
    "- pandas: Data manipulation library in Python.\n",
    "- sklearn.model_selection.train_test_split: Function for splitting data into training and testing sets.\n",
    "\n",
    "## Class: ImportData\n",
    "This class contains methods to perform the following tasks:\n",
    "\n",
    "1. **Data Typing**:\n",
    "   - Determines the type of data to import based on the provided parameter.\n",
    "\n",
    "2. **Reading and Sampling Data**:\n",
    "   - Reads the non-bot and bot dataframes from CSV files.\n",
    "   - Samples the bot data based on specified bot ratios and combines it with non-bot data.\n",
    "   \n",
    "3. **Splitting Dataset**:\n",
    "   - Splits the dataset into training, testing, and validation sets based on the provided proportions.\n",
    "   - Stratifies the split based on the target feature to maintain the distribution of classes in each split.\n",
    "\n",
    "## Methods:\n",
    "\n",
    "### `type_data(self, type_data_merged)`\n",
    "Determines the type of data to use based on the provided parameter (`type_data_merged`).\n",
    "\n",
    "### `read_and_sample_data(self, base_path=\"../Data/cresci-2015.csv/\", type_data_merged=1, bot_ratio=[.2, .8], bot_fldr_ratio=[1, 1, 1])`\n",
    "Reads and samples data from CSV files based on the provided parameters.\n",
    "- `base_path`: The base directory containing the dataset files.\n",
    "- `type_data_merged`: Type of data to use (merged or user-specific).\n",
    "- `bot_ratio`: Desired ratio of bot samples in the final dataset.\n",
    "- `bot_fldr_ratio`: Ratios of bot samples from different bot folders.\n",
    "\n",
    "### `split_dataset(self, data, proportions=[.7, .15, .15], target='bot')`\n",
    "Splits the dataset into training, testing, and validation sets.\n",
    "- `data`: The DataFrame to split.\n",
    "- `proportions`: Proportions of training, testing, and validation sets.\n",
    "- `target`: The name of the target feature.\n",
    "\n",
    "## Example Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = ImportData()\n",
    "\n",
    "# Read and sample data\n",
    "data = importer.read_and_sample_data()\n",
    "print(\"Sampled Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Split dataset\n",
    "split_data = importer.split_dataset(data)\n",
    "print(\"\\nSplit Data:\")\n",
    "for key, value in split_data.items():\n",
    "    print(f\"{key}: {len(value)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
