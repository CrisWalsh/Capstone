{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Directory of clean_cresci_2015.py to sys.path\n",
    "sys.path.append(os.path.abspath(\"git/clean_cresci_2015.py\"))\n",
    "\n",
    "# import clean_cresci_2015\n",
    "from clean_cresci_2015 import clean_cresci_2015\n",
    "from import_data import ImportData\n",
    "from evaluation import Evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Cresci-2015 Dataset\n",
    "\n",
    "## Overview\n",
    "This script is designed to clean and preprocess the Cresci-2015 dataset, which consists of Twitter data related to social bots and genuine users. The script performs various data cleaning and preprocessing steps to prepare the data for further analysis or machine learning tasks.\n",
    "\n",
    "## Dependencies\n",
    "- pandas: Data manipulation library in Python.\n",
    "- numpy: Numerical computing library in Python.\n",
    "- os: Operating system interface for file operations.\n",
    "- datetime: Library for manipulating dates and times in Python.\n",
    "- sklearn.preprocessing.MinMaxScaler: Class for scaling numerical features to a specified range.\n",
    "- sklearn.model_selection.train_test_split: Function for splitting data into training and testing sets.\n",
    "\n",
    "### Methods\n",
    "- `clean_data`\n",
    "Cleans the dataset by loading data from CSV files, selecting important features, converting data types, and filling missing values.\n",
    "\n",
    "### Steps Performed:\n",
    "\n",
    "1. **Loading Data**: \n",
    "   - The script loads tweet and user data from CSV files located in subdirectories of the base directory specified as `base_directory`.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Selects relevant features from both tweet and user datasets.\n",
    "\n",
    "3. **Data Type Conversion**:\n",
    "   - Converts the 'created_at' column in the users dataset to datetime format.\n",
    "\n",
    "4. **Handling Missing Values**:\n",
    "   - Fills missing values with zeros for numeric columns in both tweet and user datasets.\n",
    "\n",
    "5. **Feature Engineering**:\n",
    "   - Calculates additional features such as 'account_age_years', 'followers_to_friends_ratio' in the users dataset.\n",
    "   - Calculates tweet-level features such as 'retweet_ratio' and 'reply_ratio'.\n",
    "\n",
    "6. **Normalization**:\n",
    "   - Scales numeric features in both tweet and user datasets to a range between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "7. **Data Merging**:\n",
    "   - Merges the tweet and user datasets on the 'user_id' and 'id' columns respectively.\n",
    "\n",
    "8. **Bot Labeling**:\n",
    "   - Adds a binary 'bot' label based on the folder name.\n",
    "\n",
    "9. **Saving Cleaned Data**:\n",
    "   - Saves the cleaned and processed dataframes as CSV files in a 'clean' subdirectory within each dataset's folder.\n",
    "\n",
    "### Returns:\n",
    "- None. The method adjusts the DataFrame in place.\n",
    "\n",
    "### Error Handling\n",
    "- **FileNotFoundError**: This error may occur if the base directory does not exist or is incorrect. Ensure the directory path is correct and accessible.\n",
    "\n",
    "- **DataError**: If there are issues in data consistency such as missing columns needed for selected features, the method will raise this error.\n",
    "\n",
    "## Example Usage:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the cleaner\n",
    "cleaner = clean_cresci_2015()\n",
    "\n",
    "# Specify the base directory if different from the default\n",
    "cleaner.clean_data(base_directory=\"git/cresci-2015.csv/\")\n",
    "\n",
    "# Clean the data\n",
    "dataset_cleaner.clean_data(base_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics and Visualization\n",
    "\n",
    "## Overview\n",
    "This script defines a class `Evaluate`. This class provides tools for assessing the performance of machine learning models on Twitter data. It includes methods for calculating various performance metrics, visualizing results, and interpreting the effectiveness of model predictions.\n",
    "\n",
    "## Dependencies\n",
    "- pandas: Data manipulation library in Python.\n",
    "- numpy: Numerical computing library in Python.\n",
    "- matplotlib.pyplot: Plotting library in Python.\n",
    "- seaborn: Statistical data visualization library based on matplotlib.\n",
    "- sklearn.metrics: Collection of metrics to evaluate the performance of machine learning models.\n",
    "\n",
    "## Class: Evaluate\n",
    "The `Evaluate` class encapsulates methods to compute detailed evaluation metrics and visualize the performance of binary classifiers:\n",
    "\n",
    "**Metrics Provided**\n",
    "1. **Accuracy**: Reflects the overall correctness of the model.\n",
    "2. **Confusion Matrix**: Computes the confusion matrix rates (True Negative Rate, False Positive Rate, False Negative Rate, True Positive Rate).\n",
    "3. **Precision**: Indicates the accuracy of positive predictions.\n",
    "4. **Recall**: Measures the ability of the model to find all relevant instances.\n",
    "5. **F1 Score**: Computes the F1 score, which is the harmonic mean of precision and recall.\n",
    "6. **Matthews Correlation Coefficient (MCC)**: Computes the MCC, which measures the correlation between predicted and true binary classifications.\n",
    "7. **Area Under the ROC Curve (AUC)**: Computes the AUC score, which measures the area under the Receiver Operating Characteristic (ROC) curve.\n",
    "\n",
    "**Visualization Methods**\n",
    "- **Plot Confusion Matrix**: Visualizes the confusion matrix in a color-coded format to aid in quick interpretation.\n",
    "- **Plot ROC Curve**: Illustrates the diagnostic ability of the binary classifier system as its discrimination threshold varies.\n",
    "\n",
    "Additionally, the class provides methods to:\n",
    "- Get all evaluation metrics at once.\n",
    "- Plot the confusion matrix.\n",
    "- Plot the ROC curve.\n",
    "\n",
    "## Usage\n",
    "1. Instantiate the `Evaluate` class with true values and predicted values.\n",
    "2. Optionally, provide predicted probabilities for computing AUC.\n",
    "3. Call individual metric methods or use `get_all_metrics` to obtain all metrics at once.\n",
    "4. Use `plot_confusion_matrix` to visualize the confusion matrix.\n",
    "5. Use `plot_roc_curve` to visualize the ROC curve.\n",
    "\n",
    "## Example Usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = [0, 1, 1, 0, 1, 0, 0, 1]\n",
    "predicted_values = [0, 1, 1, 0, 1, 0, 1, 1]\n",
    "predicted_probabilities = [0.1, 0.9, 0.7, 0.2, 0.8, 0.3, 0.6, 0.4]\n",
    "\n",
    "evaluator = Evaluate(true_values, predicted_values, predicted_probabilities)\n",
    "print(evaluator.get_all_metrics())\n",
    "\n",
    "evaluator.plot_confusion_matrix()\n",
    "evaluator.plot_roc_curve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Splitting\n",
    "\n",
    "## Overview\n",
    "This script provides functionality for importing data from the Cresci-2015 dataset and splitting it into training, testing, and validation sets. It also allows for sampling the data based on specified bot ratios.\n",
    "\n",
    "## Dependencies\n",
    "- os: Operating system interface for file operations.\n",
    "- pandas: Data manipulation library in Python.\n",
    "- sklearn.model_selection.train_test_split: Function for splitting data into training and testing sets.\n",
    "\n",
    "## Class: ImportData\n",
    "This class contains methods to perform the following tasks:\n",
    "\n",
    "1. **Data Typing**:\n",
    "   - Determines the type of data to import based on the provided parameter.\n",
    "\n",
    "2. **Reading and Sampling Data**:\n",
    "   - Reads the non-bot and bot dataframes from CSV files.\n",
    "   - Samples the bot data based on specified bot ratios and combines it with non-bot data.\n",
    "   \n",
    "3. **Splitting Dataset**:\n",
    "   - Splits the dataset into training, testing, and validation sets based on the provided proportions.\n",
    "   - Stratifies the split based on the target feature to maintain the distribution of classes in each split.\n",
    "\n",
    "## Methods:\n",
    "\n",
    "### `load_data(file_path)`\n",
    "Loads data from a specified CSV file into a pandas DataFrame.\n",
    "- `file_path` (str): Path to the CSV file.\n",
    "\n",
    "**Returns:**\n",
    "- `DataFrame`: A pandas DataFrame containing the loaded data.\n",
    "\n",
    "### `type_data(self, type_data_merged)`\n",
    "Determines the type of data to use based on the provided parameter (`type_data_merged`).\n",
    "\n",
    "### `read_and_sample_data(self, base_path=\"../Data/cresci-2015.csv/\", type_data_merged=1, bot_ratio=[.2, .8], bot_fldr_ratio=[1, 1, 1])`\n",
    "Reads and samples data from CSV files based on the provided parameters.\n",
    "- `base_path`: The base directory containing the dataset files.\n",
    "- `type_data_merged`: Type of data to use (merged or user-specific).\n",
    "- `bot_ratio`: Desired ratio of bot samples in the final dataset.\n",
    "- `bot_fldr_ratio`: Ratios of bot samples from different bot folders.\n",
    "\n",
    "### `split_dataset(self, data, proportions=[.7, .15, .15], target='bot')`\n",
    "Splits the dataset into training, testing, and validation sets.\n",
    "- `data` (DataFrame): The DataFrame to split.\n",
    "- `proportions` (list): Proportions of training, testing, and validation sets.\n",
    "- `target` (str): The name of the target feature.\n",
    "\n",
    "**Returns:**\n",
    "- `dict`: A dictionary containing 'X_train', 'X_test', 'X_val', 'y_train', 'y_test', and 'y_val' DataFrames/Series.\n",
    "\n",
    "## Example Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = ImportData()\n",
    "\n",
    "# Read and sample data\n",
    "data = importer.read_and_sample_data()\n",
    "print(\"Sampled Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Split dataset\n",
    "split_data = importer.split_dataset(data)\n",
    "print(\"\\nSplit Data:\")\n",
    "for key, value in split_data.items():\n",
    "    print(f\"{key}: {len(value)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
