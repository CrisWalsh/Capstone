{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Append the directory of clean_cresci_2015.py to sys.path\n",
    "sys.path.append(os.path.abspath(\"../Code\"))\n",
    "\n",
    "# Main libraries\n",
    "from import_data import ImportData\n",
    "from evaluation import Evaluate \n",
    "from feature_selection import FeatureSelection\n",
    "from models_test import ModelTester"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS PARAMETERS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parametres is the default value of each model, parametres can be optimized before a prediction using Cross Validation  \n",
    "Otherwise the parametres can be modified here and tested on each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_parametres = {\n",
    "    'decision_tree': {\n",
    "        'ccp_alpha': 0.0,\n",
    "        'class_weight': None,\n",
    "        'criterion': 'entropy',\n",
    "        'max_depth': 20,\n",
    "        'max_features': None,\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impurity_decrease': 0.0,\n",
    "        'min_samples_leaf': 1,\n",
    "        'min_samples_split': 10,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'random_state': None,\n",
    "        'splitter': 'random'\n",
    "        },\n",
    "\n",
    "    'knn': {\n",
    "        'algorithm': 'auto',\n",
    "        'leaf_size': 10,\n",
    "        'metric': 'minkowski',\n",
    "        'metric_params': None,\n",
    "        'n_jobs': None,\n",
    "        'n_neighbors': 3,\n",
    "        'p': 1,\n",
    "        'weights': 'uniform'\n",
    "        },\n",
    "\n",
    "    'logistic_regression': {\n",
    "        'C': 0.001,\n",
    "        'class_weight': None,\n",
    "        'dual': False,\n",
    "        'fit_intercept': True,\n",
    "        'intercept_scaling': 1,\n",
    "        'l1_ratio': None,\n",
    "        'max_iter': 50,\n",
    "        'multi_class': 'auto',\n",
    "        'n_jobs': None,\n",
    "        'penalty': 'l2',\n",
    "        'random_state': None,\n",
    "        'solver': 'newton-cg',\n",
    "        'tol': 0.0001,\n",
    "        'verbose': 0,\n",
    "        'warm_start': False\n",
    "        },\n",
    "\n",
    " 'svm': {\n",
    "    'C': 1000,\n",
    "    'break_ties': False,\n",
    "    'cache_size': 200,\n",
    "    'class_weight': 'balanced',\n",
    "    'coef0': 0.1,\n",
    "    'decision_function_shape': 'ovr',\n",
    "    'degree': 2,\n",
    "    'gamma': 'scale',\n",
    "    'kernel': 'poly',\n",
    "    'max_iter': -1,\n",
    "    'probability': True,\n",
    "    'random_state': None,\n",
    "    'shrinking': True,\n",
    "    'tol': 0.001,\n",
    "    'verbose': False\n",
    "  }\n",
    "  }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_results(\n",
    "    model_parametres,\n",
    "    test_metrics,\n",
    "    val_metrics,\n",
    "    DATASET,\n",
    "    BOT_FOLDERS,\n",
    "    BOT_RATIO,\n",
    "    MERGED_DATASET,\n",
    "    TYPE_SELECTION,\n",
    "    TRAIN_RATE,\n",
    "    TEST_RATE,\n",
    "    VAL_RATE,\n",
    "    MODEL,\n",
    "    FEATURES\n",
    "):\n",
    "    # Expand the dictionaries with appropriate prefixes\n",
    "    data = {\n",
    "        \"DATASET\": DATASET,\n",
    "        \"BOT_FOLDERS\": str(BOT_FOLDERS),\n",
    "        \"BOT_RATIO\": str(BOT_RATIO),\n",
    "        \"MERGED_DATASET\": MERGED_DATASET,\n",
    "        \"TYPE_SELECTION\": TYPE_SELECTION,\n",
    "        \"TRAIN_RATE\": TRAIN_RATE,\n",
    "        \"TEST_RATE\": TEST_RATE,\n",
    "        \"VAL_RATE\": VAL_RATE,\n",
    "        \"MODEL\": MODEL,\n",
    "        \"FEATURES\": FEATURES,\n",
    "        **{f\"test_{k}\": v for k, v in test_metrics.items()},\n",
    "        **{f\"val_{k}\": v for k, v in val_metrics.items()},\n",
    "        **model_parametres\n",
    "    }\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    df = pd.DataFrame([data])\n",
    "    csv_file_name = f\"{MODEL}_results.csv\"\n",
    "\n",
    "    # Check if the CSV file already exists\n",
    "    if os.path.exists(csv_file_name):\n",
    "        # Load existing data\n",
    "        existing_df = pd.read_csv(csv_file_name)\n",
    "\n",
    "        # Concatenate new data with old data\n",
    "        updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "        # Drop duplicates\n",
    "        updated_df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "        # Save the updated DataFrame to CSV\n",
    "        updated_df.to_csv(f\"../Outputs/{csv_file_name}\", index=False)\n",
    "    else:\n",
    "        # If the file does not exist, save the DataFrame as new file\n",
    "        df.to_csv(f\"../Outputs/{csv_file_name}\", index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main features to choose\n",
    "DATASET = 'clean_data'\n",
    "BOT_FOLDERS = [1, 1, 1] # Might be different between cresci_2015 and cresci_2017\n",
    "BOT_RATIO = [.35, .65] # Non-bot to Bot\n",
    "MERGED_DATASET = True # Merged dataset uses user info plus tweets \n",
    "TYPE_SELECTION = \"correlation\" # correlation, chi2, classification\n",
    "TRAIN_RATE = .7\n",
    "TEST_RATE = .15\n",
    "VAL_RATE = .15\n",
    "MODEL = 'all'\n",
    "FEATURES = None # none equals to test all features, otherwise enter a number of features\n",
    "MODEL_P = None # Only use the template provided on top for modifying parametres for test\n",
    "GRID_SEARCH = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'total_non_bot' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Import the data \u001b[39;00m\n\u001b[1;32m      2\u001b[0m importer \u001b[39m=\u001b[39m ImportData()\n\u001b[0;32m----> 3\u001b[0m data \u001b[39m=\u001b[39m importer\u001b[39m.\u001b[39;49mread_and_sample_data(type_data_merged \u001b[39m=\u001b[39;49m MERGED_DATASET, \n\u001b[1;32m      4\u001b[0m                                      bot_ratio\u001b[39m=\u001b[39;49m BOT_RATIO, \n\u001b[1;32m      5\u001b[0m                                      bot_fldr_ratio\u001b[39m=\u001b[39;49m BOT_FOLDERS\n\u001b[1;32m      6\u001b[0m                                      )\n\u001b[1;32m      8\u001b[0m \u001b[39m# Do a selection of features \u001b[39;00m\n\u001b[1;32m      9\u001b[0m selection \u001b[39m=\u001b[39m FeatureSelection(data)\n",
      "File \u001b[0;32m~/Capstone/Code/import_data.py:43\u001b[0m, in \u001b[0;36mImportData.read_and_sample_data\u001b[0;34m(self, base_path, type_data_merged, bot_ratio, bot_fldr_ratio)\u001b[0m\n\u001b[1;32m     40\u001b[0m         total_bot \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(bot_data_frames)\n\u001b[1;32m     42\u001b[0m \u001b[39m# Calculate available ratios\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m total_samples \u001b[39m=\u001b[39m total_bot \u001b[39m+\u001b[39m total_non_bot\n\u001b[1;32m     44\u001b[0m required_bot_samples \u001b[39m=\u001b[39m total_samples \u001b[39m*\u001b[39m bot_ratio[\u001b[39m1\u001b[39m]\n\u001b[1;32m     45\u001b[0m required_non_bot_samples \u001b[39m=\u001b[39m total_samples \u001b[39m*\u001b[39m bot_ratio[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'total_non_bot' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Import the data \n",
    "importer = ImportData()\n",
    "data = importer.read_and_sample_data(type_data_merged = MERGED_DATASET, \n",
    "                                     bot_ratio= BOT_RATIO, \n",
    "                                     bot_fldr_ratio= BOT_FOLDERS\n",
    "                                     )\n",
    "\n",
    "# Do a selection of features \n",
    "selection = FeatureSelection(data)\n",
    "list_features = selection.select_features(type_selection = TYPE_SELECTION)\n",
    "\n",
    "# Create the splits \n",
    "SPLIT_RATES = [TRAIN_RATE, TEST_RATE, VAL_RATE] \n",
    "splits = importer.split_dataset(data = data, \n",
    "                                proportions= SPLIT_RATES\n",
    "                                )\n",
    "\n",
    "# Test Model \n",
    "test_enviroment = ModelTester(splits, list_features)\n",
    "\n",
    "\n",
    "############################################## ALL MODELS PREDICTION ##################################################################\n",
    "if MODEL =='all':\n",
    "    # Multiple models results DF \n",
    "    results = {}\n",
    "\n",
    "    # Loop across all models\n",
    "    for model in test_enviroment.models.keys():\n",
    "        # Generate the predictions\n",
    "        predictions = {}\n",
    "\n",
    "        # check if model parametres need to be changed\n",
    "        if MODEL_P != None:\n",
    "            test_enviroment.change_model_parameters(model_name=model, \n",
    "                                                    new_params=models_parametres[model]\n",
    "                                                    )\n",
    "\n",
    "        # check if model parametres need to be optimized before prediction\n",
    "        if (MODEL_P == None) and (GRID_SEARCH == True):\n",
    "            test_enviroment.grid_search(model_name = model, \n",
    "                                        num_features = FEATURES)\n",
    "\n",
    "        # Get the current parametres for prediction\n",
    "        model_parametres = test_enviroment.models[model].get_params()\n",
    "        \n",
    "        # Create a prediction    \n",
    "        predictions = test_enviroment.predict_model(model_name = model, \n",
    "                                                    num_features= FEATURES\n",
    "                                                    )\n",
    "\n",
    "        # Evaluate the predictions for Test Dataset\n",
    "        val_evaluation = Evaluate(true_values=splits['y_val'], \n",
    "                           predicted_values= predictions['val_predictions'], \n",
    "                           predicted_probabilities= predictions['val_probabilities']\n",
    "                           )\n",
    "        val_metrics = val_evaluation.get_all_metrics()\n",
    "\n",
    "        # Evaluate the predictions for Test Dataset\n",
    "        test_evaluation = Evaluate(true_values=splits['y_test'], \n",
    "                           predicted_values= predictions['test_predictions'], \n",
    "                           predicted_probabilities= predictions['test_probabilities']\n",
    "                           )\n",
    "        test_metrics = test_evaluation.get_all_metrics()\n",
    "\n",
    "        print(model)\n",
    "\n",
    "        # Save the result\n",
    "        results[model] = save_results(\n",
    "            model_parametres=model_parametres,\n",
    "            test_metrics=test_metrics,  # Ensure the correct variable name is used\n",
    "            val_metrics=val_metrics,  # Ensure the correct variable name is used\n",
    "            DATASET=DATASET,\n",
    "            BOT_FOLDERS=BOT_FOLDERS,\n",
    "            BOT_RATIO=BOT_RATIO,\n",
    "            MERGED_DATASET=MERGED_DATASET,\n",
    "            TYPE_SELECTION=TYPE_SELECTION,\n",
    "            TRAIN_RATE=TRAIN_RATE,\n",
    "            TEST_RATE=TEST_RATE,\n",
    "            VAL_RATE=VAL_RATE,\n",
    "            MODEL=model,\n",
    "            FEATURES=FEATURES\n",
    "        )\n",
    "    \n",
    "######################################### SINGLE MODEL PREDICTION ########################################################################\n",
    "else:\n",
    "    # Generate the predictions\n",
    "    predictions = {}\n",
    "\n",
    "    # check if model parametres need to be changed\n",
    "    if MODEL_P != None:\n",
    "        test_enviroment.change_model_parameters(model_name=MODEL, \n",
    "                                                new_params=models_parametres[MODEL]\n",
    "                                                )\n",
    "\n",
    "    # check if model parametres need to be optimized before prediction\n",
    "    if (MODEL_P == None) and (GRID_SEARCH == True):\n",
    "        test_enviroment.grid_search(model_name = MODEL, \n",
    "                                    num_features = FEATURES)\n",
    "        \n",
    "    # Get the current parametres for prediction\n",
    "    model_parametres = test_enviroment.models[MODEL].get_params()\n",
    "\n",
    "    # Generate a prediction\n",
    "    predictions = test_enviroment.predict_model(model_name = MODEL, \n",
    "                                                num_features= FEATURES)\n",
    "\n",
    "    # Evaluate the predictions for Test Dataset\n",
    "    val_evaluation = Evaluate(true_values=splits['y_val'], \n",
    "                        predicted_values= predictions['val_predictions'], \n",
    "                        predicted_probabilities= predictions['val_probabilities']\n",
    "                        )\n",
    "    val_metrics = val_evaluation.get_all_metrics()\n",
    "\n",
    "    # Evaluate the predictions for Test Dataset\n",
    "    test_evaluation = Evaluate(true_values=splits['y_test'], \n",
    "                        predicted_values= predictions['test_predictions'], \n",
    "                        predicted_probabilities= predictions['test_probabilities']\n",
    "                        )\n",
    "    test_metrics = test_evaluation.get_all_metrics()\n",
    "    \n",
    "    # Save the result\n",
    "    df = save_results(\n",
    "        model_parametres=model_parametres,\n",
    "        test_metrics=test_metrics,  # Ensure the correct variable name is used\n",
    "        val_metrics=val_metrics,  # Ensure the correct variable name is used\n",
    "        DATASET=DATASET,\n",
    "        BOT_FOLDERS=BOT_FOLDERS,\n",
    "        BOT_RATIO=BOT_RATIO,\n",
    "        MERGED_DATASET=MERGED_DATASET,\n",
    "        TYPE_SELECTION=TYPE_SELECTION,\n",
    "        TRAIN_RATE=TRAIN_RATE,\n",
    "        TEST_RATE=TEST_RATE,\n",
    "        VAL_RATE=VAL_RATE,\n",
    "        MODEL=MODEL,\n",
    "        FEATURES=FEATURES\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pd\u001b[39m.\u001b[39mset_option(\u001b[39m'\u001b[39m\u001b[39mdisplay.max_columns\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \n\u001b[0;32m----> 2\u001b[0m results[\u001b[39m'\u001b[39m\u001b[39mknn\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None) \n",
    "results['knn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
